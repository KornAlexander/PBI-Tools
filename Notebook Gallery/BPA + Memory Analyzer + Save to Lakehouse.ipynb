{"cells":[{"cell_type":"markdown","source":["This script runs the built-in Notebooks for Semantic Model Check at the same time:\n","1. Best Practice Analyzer (BPA)\n","2. Memory Analyzer (equivalent to Vertipaq Analyzer)\n","3. Added Bonus: The Resultset of the BPA + Three DAX INFO View tables are saved into a Lakehouse\n"," \n","Process:\n","1. Create Lakehouse\n","2. Add Lakehouse to Notebook\n","3. Modify in the Python script the configuration for your Lakehouse Name, Workspace Name and Semantic Model Name\n","4. Run the Script\n","5. Check the result directly in the notebook or with the SQL Endpoint in your warehouse "],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08ccb876-16aa-4dc2-b8cb-f15c7722bbf6"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","from datetime import datetime\n","import re\n","import pandas as pd\n","\n","# ============================================================================\n","# CONFIGURATION\n","# ============================================================================\n","dataset = \"Semantic Model Name\"\n","workspace = \"Workspace Name\"\n","lakehouse = \"Lakehouse_Name\"\n","\n","# Function to clean column names\n","def clean_column_name(col_name):\n","    col_name = str(col_name).replace('[', '').replace(']', '')\n","    col_name = col_name.replace(' ', '_')\n","    col_name = re.sub(r'[,;{}()\\n\\t=]', '', col_name)\n","    return col_name\n","\n","# Function to save DataFrame with smart append/overwrite\n","def save_to_lakehouse(df, table_name, description=\"results\"):\n","    \"\"\"\n","    Saves DataFrame to lakehouse. Tries append first, creates table if needed.\n","    \"\"\"\n","    full_table_name = f\"{lakehouse}.{table_name}\"\n","    \n","    # Convert pandas to Spark if needed\n","    if isinstance(df, pd.DataFrame):\n","        spark_df = spark.createDataFrame(df)\n","    else:\n","        spark_df = df\n","    \n","    print(f\"Saving {description}...\")\n","    \n","    try:\n","        # Try append first\n","        spark_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"append\") \\\n","            .option(\"mergeSchema\", \"true\") \\\n","            .saveAsTable(full_table_name)\n","        print(f\"✓ Appended {len(df)} records\")\n","    except:\n","        # Table doesn't exist, create it\n","        spark_df.write \\\n","            .format(\"delta\") \\\n","            .mode(\"overwrite\") \\\n","            .option(\"overwriteSchema\", \"true\") \\\n","            .saveAsTable(full_table_name)\n","        print(f\"✓ Created table with {len(df)} records\")\n","\n","print(\"=\"*80)\n","print(\"STEP 1: Running Best Practice Analyzer...\")\n","print(\"=\"*80)\n","\n","# Run BPA analysis\n","try:\n","    bpa_results = fabric.run_model_bpa(\n","        dataset=dataset, \n","        workspace=workspace,\n","        return_dataframe=True\n","    )\n","except TypeError:\n","    bpa_results = fabric.run_model_bpa(dataset=dataset, workspace=workspace)\n","\n","if bpa_results is not None and len(bpa_results) > 0:\n","    # Add metadata\n","    bpa_results['analysis_timestamp'] = datetime.now()\n","    bpa_results['model_name'] = dataset\n","    bpa_results['workspace_name'] = workspace\n","    bpa_results.columns = [clean_column_name(col) for col in bpa_results.columns]\n","\n","    # Save to lakehouse\n","    save_to_lakehouse(bpa_results, \"bpa_analysis_results\", \"BPA results\")\n","\n","    print(\"\\nSummary by Severity:\")\n","    display(bpa_results.groupby('Severity').size())\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"STEP 2: Running Model Memory Analyzer...\")\n","print(\"=\"*80)\n","\n","memory_results = fabric.model_memory_analyzer(dataset=dataset, workspace=workspace)\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"STEP 3: Capturing Memory Statistics via DAX...\")\n","print(\"=\"*80)\n","\n","spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n","\n","try:\n","    # Table statistics\n","    print(\"\\nGetting table statistics...\")\n","    table_stats = fabric.evaluate_dax(dataset=dataset, workspace=workspace, dax_string=\"EVALUATE INFO.TABLES()\")\n","    \n","    if table_stats is not None and len(table_stats) > 0:\n","        table_stats['analysis_timestamp'] = datetime.now()\n","        table_stats['model_name'] = dataset\n","        table_stats['workspace_name'] = workspace\n","        table_stats.columns = [clean_column_name(col) for col in table_stats.columns]\n","        \n","        # Handle old timestamps\n","        for col in ['ModifiedTime', 'RefreshedTime', 'StructureModifiedTime']:\n","            if col in table_stats.columns:\n","                table_stats[col] = pd.to_datetime(table_stats[col], errors='coerce')\n","                table_stats[col] = table_stats[col].where(table_stats[col] > pd.Timestamp('1900-01-01'), None)\n","        \n","        save_to_lakehouse(table_stats, \"memory_analysis_tables\", \"table statistics\")\n","        display(table_stats.head(10))\n","    \n","    # Column statistics\n","    print(\"\\nGetting column statistics...\")\n","    column_stats = fabric.evaluate_dax(dataset=dataset, workspace=workspace, dax_string=\"EVALUATE INFO.COLUMNS()\")\n","    \n","    if column_stats is not None and len(column_stats) > 0:\n","        column_stats['analysis_timestamp'] = datetime.now()\n","        column_stats['model_name'] = dataset\n","        column_stats['workspace_name'] = workspace\n","        column_stats.columns = [clean_column_name(col) for col in column_stats.columns]\n","        \n","        # Handle old timestamps\n","        for col in ['ModifiedTime', 'RefreshedTime', 'StructureModifiedTime']:\n","            if col in column_stats.columns:\n","                column_stats[col] = pd.to_datetime(column_stats[col], errors='coerce')\n","                column_stats[col] = column_stats[col].where(column_stats[col] > pd.Timestamp('1900-01-01'), None)\n","        \n","        save_to_lakehouse(column_stats, \"memory_analysis_columns\", \"column statistics\")\n","        display(column_stats.head(10))\n","    \n","    # Measure statistics\n","    print(\"\\nGetting measure statistics...\")\n","    measure_stats = fabric.evaluate_dax(dataset=dataset, workspace=workspace, dax_string=\"EVALUATE INFO.MEASURES()\")\n","    \n","    if measure_stats is not None and len(measure_stats) > 0:\n","        measure_stats['analysis_timestamp'] = datetime.now()\n","        measure_stats['model_name'] = dataset\n","        measure_stats['workspace_name'] = workspace\n","        measure_stats.columns = [clean_column_name(col) for col in measure_stats.columns]\n","        \n","        # Handle old timestamps\n","        for col in ['ModifiedTime', 'RefreshedTime', 'StructureModifiedTime']:\n","            if col in measure_stats.columns:\n","                measure_stats[col] = pd.to_datetime(measure_stats[col], errors='coerce')\n","                measure_stats[col] = measure_stats[col].where(measure_stats[col] > pd.Timestamp('1900-01-01'), None)\n","        \n","        save_to_lakehouse(measure_stats, \"memory_analysis_measures\", \"measure statistics\")\n","        display(measure_stats.head(10))\n","        \n","except Exception as e:\n","    print(f\"Error: {str(e)}\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"✓ ANALYSIS COMPLETE!\")\n","print(\"=\"*80)\n","print(\"✓ All results saved to lakehouse and queryable via SQL Analytics Endpoint\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a2486a67-d45b-446d-8a47-d89debf89c7a"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}